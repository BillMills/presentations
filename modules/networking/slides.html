<section data-background="images/title-whale.png" class="blue_bg">
	<h2>Docker Networking</h2>
	<aside class="notes"></aside>
</section>
                
<section data-background="#1AAAF8" class="blue_bg">
    <h2>Module Objectives</h2>
    <ul>
      <li>Introduce Docker networking goals</li>
      <li>Understand the Container Network Model</li>
      <li>Understand the role of Docker network drivers</li>
      <li>Deep dive into bridge & overlay networks</li>
      <li>Physical networks</li>
      <li>Service discovery</li>
      <li>Load balancing</li>
      <li>Security</li>
    </ul>  
    <aside class="notes">
    	These are the topics we’ll try to touch on in the next n hours. To start - what are the key concerns a service network needs to deliver on? (discuss)
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Docker Networking</h2>
    <p>prioritizes service discovery, load balancing, security, performance, scalability, <b>but most importantly portability</b>.</p>
    <aside class="notes">
		In some sense, the first 5 are tablestakes for any distributed application; Docker’s special sauce is the last one - portability across and between infrastructure. What abstractions are necessary to achieve network portability in practise? (discuss; lead students to think about network virtualization and container network isolation).
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>The Container Network Model</h2>
    <img style='max-width: 80%' src='images/cnm.png'></img>
    <aside class="notes">
    	at high level, all container networks follow a model consisting of three parts:
    	 - the network sandbox, which on linux machines is exactly a linux network namespace. This is a security feature of container networking that provides independent routing tables and network interfaces to each container. This provides the network isolation we decided we wanted above.
    	 - the endpoint, which provides a channel for traffic to and from the network namespace. In practice, this typically appears as eth0 (etc) inside the contianer, and accepts an external veth connection (veth == virtual ethernet connection, used to route traffic between network namespaces). The endpoint serves to abstract network details away from the application logic, so that different networks can be swapped out easily.
    	 - the network itself, an object whose only contract is to provide connectivity between containers; an example of this that we'll dig into in detail in a moment is a linux bridge.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Network Drivers</h2>
    <ul>
    	<li>Define the behavior of the network in the CNM</li>
    	<li>Are container agnostic thanks to the endpoint abstraction</li>
    	<li>Complete list: see the docs, but we will focus on <b>bridge</b> and <b>overlay</b>.</li>
    </ul>
    <aside class="notes">
    	All the fun in the CNM comes from the network drivers. You may have noticed that the contract for networks in the CNM is kind of trivial - anything that connects containers counts. Correspondingly, there's a bunch of different drivers out there, and you can even create your own; today we're going to focus on the two workhorses of docker networking - bridge, and overlay.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>The Bridge Network</h2>
    <img style='width: 45%; float:left;' src='images/bridge1.png'></img>
    <div style='float:right; max-width: 50%;'>
    	<h3>Birth of a Bridge Net:</h3>
    	<pre>docker network create -d bridge my_bridge ...</pre>
    	<ul>
    		<li>Engine triggers driver to create a network</li>
    		<li>Bridge driver instantiates Linux bridge & sets up iptables</li>
    	</ul>
    	<pre>docker run --net my_bridge</pre>
    	<ul>
    		<li>driver spawns veth</li>
    		<li>ends of veth plugged into container endpoint and linux bridge</li>
    	</ul>
    </div>
    <aside class="notes">
    	- simplest nontrivial network is the Bridge network<br>
    	- uses a linux bridge as the CNM's network (linux bridges are a virtualization of a network switch, providing level 2 routing via MAC address)<br>
    	- (walk through birth of bridgenet)<br>
    	- note that for linux networking fans, there's nothing actually new here: a linux bridge is plugged into a network namespace via a veth; in some sense, container networking IS linux networking.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Default Bridge Routing Demo</h2>
    <p>SSH into any machine with docker installed, and try the following:</p>
    <pre>
    	host> docker run -it --name c1 busybox sh
    	c1> ip address   # look for eth0; compare MAC to IP
    	host> sudo apt-get install bridge-utils
    	host> brctl show
    	c1> route -n
    	host> route -n
    </pre>
    <aside class="notes">
    	 - start by standing up any old container with no network specifics defined at all<br>
    	 - note the container MAC addresses are just encodings of their IP, to avoid collisions.<br>
    	 - docker always sets up a default network called `bridge`, which consists of a linux bridge called `docker0`. By default, all new containers are plugged into their host's `docker0` bridge, so they can talk to each other.<br>
    	 - back inside the container, we can see traffic is directed to eth0, and thus externally down the veth and to the bridge.<br> 
    	 - additionally, `docker0` appears as a device on the host's routing table.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>User-Defined Bridge Network Demo</h2>
    <p>On the same node as before:</p>
    <pre>
    	host> docker network create -d bridge --subnet 10.0.0.254/16 my_bridge
    	host> docker run -itd --name c2 --net my_bridge busybox sh
    	host> docker run -itd --name c3 --net my_bridge --ip 10.0.0.254 busybox sh
    	host> brctl show
    	host> docker network ls # compare network ID and bridge name
    	host> ip link	
    </pre>
    <aside class="notes">
    	 - start by creating a simple bridge network<br>
    	 - stand up a container on your network, with no other specifics<br>
    	 - stand up another container with a user-defined ip<br>
    	 - your new bridge has two veths plugged into it, one for each container<br>
    	 - note the default network ID matches the bridge name.<br>
    	 - all Docker has done is stand up a bunch of linux devices

         - REMOVE all this stuff after the example; fixed subnet can cause weird collisions.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>User Defined Bridges</h2>
    <img style='max-width: 80%' src='images/bridge2.png'></img>
    <p>Note that bridge networks are firewalled by default.</p>
    <aside class="notes">
    	- here's the setup you built over the last two demos.<br>
    	- one key feature of bridge networks and most docker networks is that they are firewalled from other networks by default. Containers can communicate across their namespaces via their shared linux bridges on the same network, but are isolated from each other otherwise.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>External Connectivity</h2>
    <img style='max-width: 80%' src='images/nat.png'></img>
    <p>Bridge networks are not exposed on the external network, but containers can be port mapped.</p>
    <aside class="notes">
    	- in addition to firewalling against other networks, bridge networks are not accessible by default from the external network; containers have IPs assigned to them from the bridge's subnet, and that's it.<br>
    	- containers are allowed by default to send egress traffic along a random port mapping, and receive a response along the same.<br>
    	- ports can also be explicitly mapped from container to host via a --publish directive at the cli, mappings in a Dockerfile or Compose file, or via UCP.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Exercise: Container Reachability</h2>
    <ul>
    	<li>From container c3 that you created in the demo, try <pre>ping c2</pre></li>
    	<li>Also from c3, try <pre>nslookup c2</pre>; do the results make sense?</li>
    	<li>Try the same operations to or from c1; discuss with the people sitting near you.</li>
    </ul>
    <aside class="notes">
    	- docker exec c3 ping c2 <br>
    	- docker exec c3 nslookup c2<br>
    </aside>
</section>

<!--section data-background="#1AAAF8" class="blue_bg">
    <h2>Exercise: Network Aliasing</h2>
    <p>Discuss each step with your neighbours; try to explain the results you get to each other.</p>
    <ul>
    	<li>Create a couple of new containers, this time with network aliases: 
    		<pre>
    			docker run -itd --name c4 --net my_bridge –net-alias foo busybox sh
    			docker run -itd --name c5 --net my_bridge –net-alias bar busybox sh
    		</pre>
    	</li>
    	<li>Ping c4 from c5.</li>
    	<li>Now try `ping foo` from c5; discuss.</li>
    	<li>Create another container c6 on the foo alias.</li>
    	<li>Ping foo again from c5, and discuss.</li>
    	<li>What does `nslookup foo` give from c5?</li>
    </ul>
    <aside class="notes">

    </aside>
</section-->

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Multi-Host Networking</h2>
    <p>Docker Swarm allows the distribution of containers and services across any number of hosts; what are the fundamental requirements for a container on one host to communicate with a container on another host?</p>
    <aside class="notes">
    	Given what we’ve seen so far with firewalled container networks addressed by private, network specific subnets, what would need to happen to successfully communicate in a multi-host setting? (discuss; lead class to thinking about traversing the external network starting from an address in a local subnet (ie vxlan), particularly in the context of linux bridges trying to do level 2 communications). Previously the linux bridge could just bounce L2 packets around as it pleased, but now there's this whole L3 network to traverse.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>VXLAN Packet Walk</h2>
    <img style='max-width: 80%' src='images/packetwalk.png'></img>
    <aside class="notes">
    	 - We can achieve this via virtual extensible LAN, or VXLAN. VXLAN is an encapsulation standard that takes L2 communications and wraps them in headers that allow their transport over the L3 network, which is exactly what this figure is trying to illustrate.<br>
    	 - c1 does a DNS lookup for c2. Since both containers are on the same overlay network the Docker Engine local DNS server resolves c2 to its overlay IP address 10.0.0.3.<br>
    	 - We do these communications with linux bridges, so c1 generates an L2 frame destined for the MAC address of c2.<br>
    	 - The frame is encapsulated with a VXLAN header by the overlay network driver. The distributed overlay control plane knows that c2 resides on host-B at the physical address of 192.168.0.3.<br>
    	 - Once encapsulated the packet is sent across the physical network.<br>
    	 - The packet arrives at the eth0 interface of host-B and is decapsulated by the overlay network driver. The original L2 frame from c1 is passed to the c2's eth0 interface and up to the listening application.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>VXLAN Packet Walk</h2>
    <img style='max-width: 80%' src='images/packetwalk-nigel.png'></img>
    <aside class="notes">
    	- In a little more detail, overlay networks share a lot of the plumbing as bridge networks. Containers still have a veth plugging them into a linux bridge; what's new here is the VXLAN Tunnel Endpoint (VTEP) hanging out on each host; these are the devices responsible for doing the wrapping and unwrapping illustrated previously.<br>
    	- Let's walk through C1 trying to ping C2 in the figure:<br>
    	- C1 does its DNS lookup for C2, and finds its IP of 10.0.0.4<br>
    	- packet goes to the linux bridge, as usual<br>
    	- the first time, the linux bridge won't have an entry for the MAC address of C2, so it floods all ports with the packet.<br>
    	- the VTEP catches this and responds with its own MAC address - now the bridge will send all such traffic to the VTEP<br>
    	- the VTEP on node1 encapsulates the ping with the headers it needs to traverse the physical network and arrive at UDP port 4789.<br>
    	- the VTEP on node2 decapsulates the ping and sends it to the local linux bridge, which recognizes C2's MAC address and delivers the packet.<br>
    	- Voila! But there was a little bit of magic here - what step did I skip? (discuss; lead students to realize that the VTEP on host1 doesn't apriori know anything about what host C2 lives on; routing tables need to be syncronized)
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Multi-Host Control Plane Gossip Network</h2>
    <img style='max-width: 80%' src='images/gossip.png'></img>
    <p>All swarm workers participate in a gossip network to update each other's network control planes.</p>
    <aside class="notes">
    	- In order for multi-host networking to function, every host needs to know where every container is in an up to date routing table.<br>
    	- The number of connections blows up like n^2 - not good if we want our networks to scale nicely.<br>
    	- All worker nodes therefore participate in a gossip network to syncronize control planes.<br>
    	- Every communication transmits state to exactly 3 peers, reaching eventual consistency<br>
    	- slow full node replication to resolve inconsistencies<br>
    	- network topology aware to pick peers with low latency<br>
    	- all inter-node control-plane communication TLS encrypted by default<br>
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Overlay Network Internal Arch</h2>
    <img style='max-height: 80%; float: left;' src='images/overlayarch.png'></img>
    <p style='float:right;'>In addition to the overlay network itself, all hosts participating in a multi-host network have exactly one bridge called docker_gwbridge, for egress traffic.</p>
    <aside class="notes">
    	- everything we've talked about so far happens on this overlay network; but under the hood, there's actually a second bridge per host called `docker_gwbridge`, which facilitates container egress traffic; container-to-container communication is blocked on this bridge
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Overlay Network Demo</h2>
    <p>Try setting up your own overlay network with one container plugged in, and investigate the network structure from inside the container:</p>
    <pre>
        host> docker swarm init
    	host> docker network create -d overlay ovnet
    	host> docker service create --replicas 1 --network ovnet --name container nginx
    	host> docker exec -it <container id> ip link
    </pre>
    <aside class="notes">
    	 - ip link shows the network connections from the perspective of the nginx service; the overlay network is the eth connection with ip 0a.x.y.z, and the gwbridge should be ac.x.y.z.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Physical Networks</h2>
    <p>Notice the none of the preceeding discussion relied on:</p>
    <ul>
    	<li>Multicast</li>
    	<li>External key/value store</li>
    	<li>Specific routing protocols</li>
    	<li>Specific network topology</li>
    </ul>
    <aside class="notes">
    	 - We've seen bridge and overlay networks; there are more built in network drivers and you can even roll your own, but bridge and overlay are your real workhorses. If you've used Compose before, you've seen a bridge network in action; if you've used Swarm, that was an overlay network under the hood.<br>
    	 - We advertised at the beginning that a key priority for Docker networks, like all Docker products, is portability across network infrastructure. Note all the things you DON'T need to make any of this work. Almost everything Docker core drivers need to work are part of the Linux kernel - and everything else is baked into Engine.<br>
    	 - Multicast has scaling problems at enterprise-sized deployments; gossip dodges that bullet; old hands at Docker remember when multi-host networks needed an external KV, but we've baked that right into Engine as of 1.12; and Docker glides right past any routing protocol or network topology you like thanks to linux bridges on the host and vxlan wrappers across the L3 network.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Service Discovery</h2>
    <img style='max-width: 80%' src='images/DNS.png'></img>
    <aside class="notes">
    	- We've seen Docker network service discovery in action already when we tried pinging one container from another, but this diagram lays it out in a litte more explicit detail.<br>
    	- Every instance of Docker Engine, and therefore every host, has an internal DNS server that containers forward DNS lookups to. This DNS server is one of the things that the multi-host gossip network keeps synced across hosts.<br>
    	- In the figure, the task1 container fires off a couple of curls; all DNS lookups proceed to the container's 127.0.0.11:53 to begin with. The curl to docker.com isn't listed as a container or a service in the Engine's DNS table, so it gets forwarded to the external DNS and the wide world.<br>
    	- The curl to myservice, however, hits an entry in the Engine's DNS table, which returns the local subnet address that the container can be found at, and reached via the linux bridge on this host (or the vxlan connection to a different host, if this was a multi-host setup).<br>
    	- Also note that looking up containers by service name is automatically load-balanced; traffic to the virtual IP 10.0.0.3 will get split among the actual service containers at 10.0.0.4 and 10.0.0.5 via the IP virtual server (IPVS), once again built off of the native linux kernel device. It's also possible to configure a service to lookup in DNS round robin mode, where the containers' actual IPs will be returned directly, rotating through the list in RR fashion.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Exercise: Service Discovery</h2>
    <img style='max-width: 80%' src='images/DNS.png'></img>
    <p>Set up a configuration of services that matches the diagram; use an `nginx` image for both services. Try installing `host` (apt-get update; apt-get install host) in the `client` container, and doing `host myservice`. Finally, stop the `myservice` service, and restart it with the flag `--endpoint-mode dnsrr`, and query its IP from `client` again.</p>
    <aside class="notes">
         docker service create --replicas 2 --name myservice --network ovnet nginx
         docker service create --replicas 1 --name client --network ovnet nginx
         docker exec -it <client container id> /bin/bash
         apt-get update
         apt-get install host
         host myservice
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>External Load Balancing: Mesh Nets</h2>
    <pre>
   		#Create a service with two replicas and export port 8000 on the cluster
		$ docker service create --name app --replicas 2 --network appnet -p 8000:80 nginx
    </pre>
    <img style='max-width: 80%' src='images/routing-mesh.png'></img>
    <aside class="notes">
    	- We've seen internal load balancing in the last couple of slides; Engine's DNS will return a virtual IP that the per-host IPVS knows how to balance across.<br>
    	- Similarly in a multi-host network, Engine 1.12 has introduced automatic load balancing for ingress traffic. When standing up a service with Swarm, the `-publish` flag publishes a service to a given port on *every* host in the cluster - whether that particular host is running the service or not.<br>
    	- When traffic is directed to this port, the particular host's IPVS will redistribute it to a node with a helathy instance of that service as per the internal load balancing.<br>
    	- All these redirects happen over a dedicated overlay network called `ingress`.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Exercise: External Load Balancing</h2>
    <pre>
   		#Create a service with two replicas and export port 8000 on the cluster
		$ docker service create --name app --replicas 2 --network appnet -p 8000:80 nginx
    </pre>
    <img style='max-width: 80%' src='images/routing-mesh.png'></img>
    <p>Set up a configuration that matches the diagram. Determine which node isn't running an nginx container, and visit port 8000 at its IP.</p>
    <aside class="notes">
        docker swarm init / join two nodes<br>
        docker network create -d overlay appnet<br>
        docker service create --name app --replicas 2 --network appnet -p 8000:80 nginx<br>
        docker service ps app<br>
        (visit each node.)
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Data Plane Security</h2>
    <pre>
    	docker network create -d overlay --opt encrypted=true <NETWORK_NAME>
    </pre>
    <img style='max-width: 80%' src='images/ipsec.png'></img>
    <aside class="notes">
    	- Control plane communications (gossip syncronization and manager->worker task assignment) is always mutually TLS encrypted, absolutely no setup required; data plane encryption can be optionally turned on with this flag.<br>
    	- Data plane encryption sticks an IPSec layer in front of the physical network connection on all nodes joined to the Swarm; Swarm managers periodically regenerate keys distributed to all hosts on the overlay network.
    </aside>
</section>

<section data-background="#1AAAF8" class="blue_bg">
    <h2>Conclusion: Priorities Revisited</h2>
    <ul>
    	<li><b>Portability</b>: VXLAN, linux kernel networking features, CNM abstraction</li>
    	<li><b>Service Discovery</b>: IPVS, gossip network</li>
    	<li><b>Load Balancing</b>: IPVS, mesh net</li>
    	<li><b>Security</b>: default mutual TLS (control plane); optional IPSec layer (data plane)</li>
    	<li><b>Performance & Scalability</b>: gossip net w/ topology aware communication; service abstraction</li>
    </ul>
    <aside class="notes">
    	- We started out by discussing our priorities for container networks: portability, service discovery, load balancing, security, performance and scalability.<br>
    	- We walked through the two big workhorses of container networking, bridge and overlay networks, and followed packets around both, and in so doing investigated the specific strategies Docker uses to deliver on these priorities (discuss).<br>
    	- Above all, the take home lesson from all this is that Docker networking heavily leverages the battle-tested networking features of the linux kernel - bridges, vxlan, iptables, veth, network namespaces, along with the high-level abstraction of the container networking model, to build networks that can be smoothly and automatically deployed and scaled on any infrastructure.
    </aside>
</section>